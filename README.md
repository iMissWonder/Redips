---
title: Redips分布式爬虫系统-设计文档
date: 2017-06-20
author: Redips团队
---

# 1.绪论
电商网站是个体户及企业进行网上销售的平台，其数据具有重要的价值，能体现经济发展趋势，居民消费水平等。但由于其数据变化极快、不同网站数据组织不同、反爬虫机制较强、重复链接多导致数据获取困难。而分布式爬虫可以对同一个网站的同类数据，进行结构化。同时，能利用分布式的软件设计方法，实现爬虫的高效采集。
Redips借助分布式爬虫的优势，优化算法、设计更全的逻辑，实现采集高效、优化内存、数据准确、用户操作便捷的目标，以满足广大电商数据需求者的需要。
主要对京东、淘宝、1号店、豆瓣、新浪、网易、腾讯网设计了自动化爬虫。
同时考虑了电商爬虫的实践应用价值，从广度、深度、速度三方面进行了深入探究：
**广度**：从耐用消费品（手机、电脑、电器、服装、箱包、美妆等）、生活消费品（食品）、娱乐消费品（电影票）的大众参考项来设计爬取字段。
**深度**：考虑到数据量大可以提高统计学精度，对网站设计了全网爬取的功能。
**速度**；影响速度的重要因素有网页去重问题、分布式搭建等问题，对这些问题进行了逐一改进及突破。
Redips的数据可以应用于经济学（验证经济学理论模型有效性）、商学（消费者行为的识别及预测）、社会学（娱乐消费品的研究）、传播学（研究社会信息系统及其运行规律）、机器学习（变量多提高神经网络机器学习的精度）等领域。考虑到数据可移植性问题，Redips同时对采集的数据生成csv格式文件，可直接应用于Excel、Matlab、Spss、Stata、R等软件。

# 2.网络爬虫及Scrapy框架
## 2.1 网络爬虫
网络爬虫也叫网络蜘蛛、蚂蚁、自动检索工具、网络疾走，是实现自动浏览网页和网页数据抓取的计算机应用程序。其主要功能是为广大网络用户浏览网页提供索引支持。
### 2.1.1 网络爬虫的研究现状
大约在上世纪90年代初，网络爬虫就与网络同时出现了。那时候网络爬虫比较简单，互联网上的数据规模比现在要小很多，而且不需要处理现在所面对的海量网络数据。
目前互联网上的网络爬虫许多，实现方法也千差万别，然而它们的处理过程却差不多。从使用性质角度来区分，网络爬虫主要被分为两大类；一类是通用网络爬虫（Generic Crawler），也叫搜索引擎爬虫，该名字的来源主要是百度、Google等很多搜索引擎公司均研发此种爬虫，显而易见通用网络爬虫追求网络的覆盖率，其尽可能的爬取更多的网页。另一类是聚焦网络爬虫（Focused Crawler），也叫主题爬虫，该爬虫是用户根据自身的需求，自定义的爬取感兴趣的网页。本文涉及的爬虫即是电商主题的爬虫。
现在有很多成熟的爬虫框架，如Python语言开发的Scrapy、C++语言开发的Larbin、Java语言实现的Heritrix、JSpider、Nutch。Scrapy的优势在于抓取网页的速度非常快，随着Python语言的使用，Scrapy框架变得非常流行，网络上有关Scrapy的学习探究层出不穷，支持用户提取各种形式的数据。本文就是用到了Scrapy这个爬虫框架的相关技术。
### 2.1.2 聚焦爬虫的工作原理
聚焦爬虫，其工作流程比通用爬虫麻烦许多，首先还是选择一个初始URL开始，然后获取网页，获取初始URL网页中的内容时需要根据需求来提取，会经过过滤，去重，分析网页数据结构等，这里还是要维护一个未爬取的URL队列，不过该URL队列的选择必须按照用户需求的算法来选择。最后重复的爬取为爬取URL队列中的URL，直到爬取结束，或时其他中止条件结束。
## 2.2 Scrapy框架
Scrapy是使用Python编写的爬虫应用框架程序，用于下载网页内容和提取关键数据。被广泛应用于数据挖掘领域，能够为科学研究提供真实可靠的网络实验数据。设计之初是以网页为抓取目标，经过不断的更新发展和功能完善，已经发展成为数据挖掘研究领域重要的应用工具。
### 2.2.1 Scrapy框架结构
Scrapy具有简单、使用方便的特点，用户借助Scrapy可以快速浏览下载网页信息，并根据需要保存关键数据为需要的数据格式。
		Scrapy框架结构：如右图所示。
**引擎**(Scrapy Engine)：用来处理整个系统的数据流处理，触发事务。
**调度器**(Scheduler)：用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。
**下载器**(Downloader)：用于下载网页内容，并将网页内容返回给蜘蛛。
**蜘蛛**(Spiders)：蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。
**项目管道**(Item Pipeline)：负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。
**下载器中间件**(Downloader Middlewares)：位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。
**蜘蛛中间件**(Spider Middlewares)：介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。
**调度中间件**(Scheduler Middlewares)：介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。
### 2.2.2 Scrapy工作原理
爬虫从初始地址开始爬取网页，抓取过程中，新的网页链接不断加入到调度中心的待爬队列中，爬虫按照下待爬队列的顺序依次抓取网页，直到待爬队列为空。

		Scrapy工作流程如右图所示。

1. 引擎打开一个网站，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。
2. 引擎从Spider中获取第一个要爬取的URL并在调度器(Scheduler)以Request调度。
3. 引擎向调度器请求下一个要爬取的URL。
4. 调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。
5. 一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。
6. 引擎从下载器接收到Response并通过Spider中间件(输入方向)发送给Spider处理。
7. Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。
8. 引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。
9. (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。

## 2.3 Scrapy-Redis
Scrapy-Redis是Scrapy爬虫框架与Redis数据库的结合体，Scrapy对Redis的操作采用Redis-py客户端。
### 2.3.1 Redis简述
Redis提供了一些丰富的数据结构，包括lists,sets,ordered sets以及hashes，还有和Memcached类似的strings结构。Redis还提供对这些数据结构的丰富操作方法。
工作原理如右图所示。

### 2.3.2 Scrapy-Redis组成及原理
Scrapy-Redis分布式体现在多个Spider同时抓取大量requests、stats数据，存入同意的Redis Queue队列，然后通过bloom filter对访问去重，在分配给各爬虫进行爬取。
		各个组件的关系如右图所示。
**connection.py**：负责根据setting中配置实例化redis连接。被dupefilter和scheduler调用，总之涉及到redis存取的都要使用到这个模块。
**dupefilter.py**：负责执行requst的去重，实现的很有技巧性，使用redis的set数据结构。但是注意scheduler并不使用其中用于在这个模块中实现的dupefilter键做request的调度，而是使用queue.py模块中实现的queue。当request不重复时，将其存入到queue中，调度时将其弹出
**queue.py**：其作用如II所述，但是这里实现了三种方式的queue：FIFO的SpiderQueue，SpiderPriorityQueue，以及LIFI的SpiderStack。默认使用的是第二中，这也就是出现之前文章中所分析情况的原因（链接）。
**pipelines.py**：这是是用来实现分布式处理的作用。它将Item存储在redis中以实现分布式处理。另外可以发现，同样是编写pipelines，在这里的编码实现不同于文章（链接：）中所分析的情况，由于在这里需要读取配置，所以就用到了from_crawler()函数。
**scheduler.py**：此扩展是对scrapy中自带的scheduler的替代（在settings的SCHEDULER变量中指出），正是利用此扩展实现crawler的分布式调度。其利用的数据结构来自于queue中实现的数据结构。scrapy-redis所实现的两种分布式：爬虫分布式以及item处理分布式就是由模块scheduler和模块pipelines实现。上述其它模块作为为二者辅助的功能模块。
**spider.py**：设计的这个spider从redis中读取要爬的url,然后执行爬取，若爬取过程中返回更多的url，那么继续进行直至所有的request完成。之后继续从redis中读取url，循环这个过程。

# 3.Redips分布式爬虫系统的设计与实现
## 3.1电商网站的特点
1. 数据变化极快，时效性极高
2. 不同网站数据组织不同，分类标签不同
3. 网站的反爬虫机制较强
4. 每个页面被多个页面链接，重复链接多
导致电商网站采集具有以下问题：
1. 爬虫被反爬机制屏蔽
2. 采集周期较长
3. 需为不同的网站定制实现程序，进行结构化，人工成本较高
4. 页面链接去重也影响采集效率

## 3.2Redips分布式爬虫系统的设计
### 3.2.1 系统总体架构设计
系统采用分布式主从结构，设置1个Master服务器和多个Slave服务器，Master管理Redis数据库和分发下载任务，Slave部署Scrapy爬虫抓取网页和解析提取项目数据。服务器的基本环境是Ubuntu操作系统，Master服务器安装Redis数据库服务器，Slave安装Scrapy和Redis客户端。

	分布式概念图：

	数据流概念图：

使用Scrapy-Redis分布式组件对调度器进行改造，并对Scrapy-Redis的去重模块的算法进行了优化：

### 3.2.2爬虫策略设计
网络爬虫的主要任务是在互联网中爬行，下载目标网页的内容。爬取策略是爬虫在网络中爬行、处理链接和下载数据的依据，科学的爬取策略是获取数据的保障。本文网络爬虫采用深度优先爬行策略，根据设定下载网页数据。
网络爬虫从初始地址获得第一批网页链接，根据Spider中定义的目标网页地址的正则表达式以此判断链接是否为要抓取的链接，是就加入待爬队列，进行去重和排序操作后交给爬虫抓取，否则判定为无用链接被丢弃。
对于搜索爬取，新链接分为商品链接和内容页链接两类。商品链接是指包含大量该搜索商品的网页；内容页链接是指向某一商品详细内容的页面。爬虫从商品网页获取新的内容页链接，加入待爬队列进一步抓取。除了这两类链接外网页中可能有少量的广告链接，爬虫会根据url特征判定这些为无用链接，自动丢弃。
对于全网爬取，在获取商品页之前还需获取目录页。目录页是网站本身提供的索引，将其作为关键字链接到商品页爬取。

流程图如图所示。

爬虫还要进一步提取内容页的字段数据，爬虫方法：使用Xpath正则表达式去匹配需要抓取的字段数据，例如商品名称、价格、店铺名称、累计评论数、销量等内容，然后分类保存。
### 3.2.3抓取字段设计
**淘宝**
序号 | 字段名称 | 字段含义
---- | ------ | ----
1 | title | 商品名称
2 | link | 商品链接
3 | shop| 商品店铺
4 | price | 商品价格
5 | taobaoPrice | 可能有的淘宝降价后价格
6 | commentsNum | 累计评论
7 | dealDoneNum | 交易成功数


**京东：**
序号	字段名称	字段含义
1	title	店铺名
2	location	店铺所在地
3	link	店铺链接
4	shopAceScore	店铺综合评分
5	shopAceScore_comRate	店铺综合评分-与同行平均水平
6	goodsContent	商品满意度
7	goodsContent_comRate	商品满意度-与同行平均水平
8	serveAttitude	服务满意度
9	serveAttitude_comRate	服务满意度-与同行平均水平
10	transSpeed	物流速度满意度
11	transSpeed_comRate	物流速度满意度-与同行平均水平
12	goodsDescribe	商品描述满意度
13	goodsDescribe_comRate	商品描述满意度-与同行平均水平
14	backGoods	退换货处理满意度
15	backGoods_comRate	退换货处理满意度-与同行平均水平
16	afersalesDealTime	售后处理时长
17	afersalesDealTime_comRate	售后处理时长-与同行平均水平
18	dealDispute	交易纠纷率
19	dealDispute_comRate	交易纠纷率-与同行平均水平
20	backRepair	退换货返修率
21	backRepair_comRate	退换货返修率-与同行平均水平
22	violationTimes	店铺违法违规信息次数


**1号店：**
序号	字段名称	字段含义
1	title	商品名称
2	price	商品价格（json全部）
3	currentPrice	显示出来的当前价格
4	link	商品链接
5	category	商品目录
6	product_id	商品ID
7	img_link	图片链接







**豆瓣：**
序号	字段名称	字段含义
1	book_id	图书id号
2	title	名称
3	author	作者
4	isbn13	isbn号
5	publisher	出版社
6	pubdate	出版时间
7	price	价格
8	pages	页数
9	translator	翻译
10	summary	概述
11	catalog	目录
12	lookcount	读者数
13	Image	封面链接
14	Tryread	试读
15	Short_review_content	短评内容
16	Short_review_name	短评作者名字
17	Short_review_id	短评作者id
18	review_title	长评标题
19	review_link	长评链接
20	review_name	长评作者名称
21	review_id	长评作者id

### 3.2.4动态网页抓取方法设计
#### 3.2.4.1 利用JSON文件
	分析网页逻辑->得到JSON文件路径->爬取JSON文件内容
#### 3.2.4.2 模拟浏览器登陆——Selenium
**Python：** 使用Python2.7.13
**Selenium：** 自动化web测试解决方案
①通过PhantomJS启动访问一个web页面
②直接操作PhantomJS浏览器对象对web页面元素进行分析
③可以模拟用户操作
**PhantomJS：** 一个没有图形界面的浏览器
Selenium的好处是完全模拟真实浏览器环境，完全模拟基本上所有用户操作。

### 3.2.5网页去重算法设计——布隆算法
BloomFilter算法，是一种大数据排重算法。在一个数据量很大的集合里，能准确断定一个对象不在集合里；判断一个对象有可能在集合里，而且占用的空间不大。它不适合那种要求准确率很高的情况，零错误的场景。通过牺牲部分准确率达到高效利用空间的目的。
BloomFilter是由一个长度为n的bit数组S和k个hash算法组成。先使bit数组的初始值为0。添加值M：M经过k个hash算法计算后，得到：M1, M2 … Mk; 然后，使S[M1]=1,S[M2]=2... S[Mk]=1，判断值Y：Y经过k个hash算法计算后，得到：Y1,Y2... Yk。 然后，判断S[Y1],S[Y2] … S[Yk] 是否都为1。如果有一个不为1，那这个Y就一定是不存在的，以前没添加过；如果都为1，那这个Y可能存在，也可能其他值添加后，影响了这次判断的结果。
我们要做的是尽量降低正确判断的误判率，资料显示， 当 k = ln(2)* m/n 时（k是hash函数个数，m是bit数组的长度，n是加入值的个数），出错概率是最小的。当然，如果要移除值，当前的结构是没法实现的，可以通过在加一个等长的数据，存放每个bit位设置为1的次数，设置一次加1，取消一次减一。

### 3.2.6反反爬方法设计
#### 3.2.6.1 Rotate user-agents
所谓的user agent，是指包含浏览器信息、操作系统信息等的一个字符串，也称之为一种特殊的网络协议。服务器通过它判断当前访问对象是浏览器、邮件客户端还是网络爬虫。在request.headers可以查看user agent。
Rotate的目的是无限次使用有限的字符串，使得UserAgent不停变换，伪装不同的浏览器。
#### 3.2.6.2代理ip
筛选稳定可靠的代理ip，建立代理ip池，从池中随机抽取代理ip，设置当前代理，进行后续爬取。其目的是使用不同的ip地址，防止单一ip被ban。
### 3.2.7数据库储模块设计——MySQL
Scrapy支持数据存储为json、csv和xml等文本格式，用户可以在运行爬虫时设置，也可以在Scrapy工程文件的ItemPipeline文件中定义。
本系统数据的存储方式如图所示。

### 3.2.8爬虫部署设计
应用Scrapyd来部署及运行爬虫，它可以使用JSON API上传并部署爬虫项目。
部署示意图如下。

### 3.2.9爬虫状态监控系统的设计
通过获取Scrapyd服务器上爬虫的状态来监控爬虫的运行状态。

### 3.2.10相同模块自动爬取方案设计
遍历原网页文件夹，用os.listdir()函数得到所有文件名，分别拼接成绝对路径，作为爬虫起始urls列表的项。网页中需要提取的项数数量大，经过分析发现，其中店铺名、店铺所在地、店铺链接、店铺违法违规信息次数、90天内平台监控店铺服务均都有值，而店铺综合评分以及180天内店铺动态评分部分值暂无。对此进行分析：
①若店铺综合评分与同行业平均水平的值为空，则在此之后所有信息都为空；故设决策变量为1，代表数据库插入时，仅插入有值项。
②若店铺综合评分与同行业平均水平的值不为空，判断满意度数组的长度，长度为多少即为值存在的行数。因空值不能插在有值行之间，故可用此逻辑。所有空值以“暂无”存入数据库。

### 3.2.11正文自动抽取方法设计
遍历原网页文件夹，用os.listdir()函数得到所有文件名，分别拼接成绝对路径，作为爬虫起始urls列表的项。
我们发现，博客分为新浪、网易、腾讯三类。通过其文件名来区分三个网站：新浪为b开头、网易为B或C开头、腾讯为q开头，分别处理，但最终都要将提取的正文写入其同名utf-8格式的文件中。具体提取方法为：用Xpath定位到主内容类，获取其下所有的text()，再用’’.join()方法拼接成字符串。其中，腾讯和极少数网易比较特殊，其网页默认编码为GBK，需要加入解码步骤。

### 3.2.12用户界面设计——Bootstrap框架
Bootstrap是一个用于快速开发Web应用程序和网站的前端框架，基于 HTML、CSS、JAVASCRIPT。
**基本结构：** Bootstrap 提供了一个带有网格系统、链接样式、背景的基本结构。
**CSS：** Bootstrap自带以下特性：全局的 CSS 设置、定义基本的 HTML 元素样式、可扩展的 class，以及一个先进的网格系统。
**组件：** Bootstrap包含了十几个可重用的组件，用于创建图像、下拉菜单、导航、警告框、弹出框等等。
**JavaScript 插件：** Bootstrap包含了十几个自定义的jQuery插件。您可以直接包含所有的插件，也可以逐个包含这些插件。
整个软件分为一个主页和若干子页面，结构如下图所示。

## 3.3Redips分布式爬虫系统的实现
### 3.3.1爬虫的实现

### 3.3.2动态网页数据爬取的实现

### 3.3.3网页去重

### 3.3.4反反爬机制的实现

### 3.3.5相同模块自动爬取的实现

### 3.3.6正文自动抽取的实现


### 3.3.7用户界面的实现
**首页界面：**

①功能介绍：4个主要电商网站的卡片正面说明覆盖率为100%，背面分别注释了Redips的功能。

②软件介绍：软件的宗旨是追求爬虫的广度、深度、速度，Redips家用网的测试数据在每个标签中展示出。

③简单的用户指南：指导用户如何启动爬虫，以及爬取结果如何查看。

④家用网速率测试结果：平均速率及最快速率。

⑤数据统计：Redips各个网站可爬取的数据总量。

⑥任务单：用户执行完一个任务可在此处作标记。

⑦爬虫启动入口及结果查看：

### 3.3.8分布式爬虫的部署——Scrapyd

### 3.3.9爬虫状态监控系统的实现

### 3.3.10分布式爬虫系统的搭建

# 4.数据处理

# 5.系统测试
## 5.1测试数据
①京东测试数据：
②淘宝测试数据：
③1号店测试数据：
④豆瓣测试数据：
⑤正文抽取测试数据：

## 5.2测试结果
①京东测试结果：
②淘宝测试结果：
③1号店测试结果：
④豆瓣测试结果：
⑤正文抽取测试结果：

# 6.总结与展望
## 6.1全文总结

## 6.2未来展望
